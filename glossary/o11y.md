---
title: "Observability (o11y)"
category: "glossary"
---

# Observability (o11y)

Observability, often abbreviated as "o11y" (with 11 representing the number of letters between 'o' and 'y'), refers to the ability to understand a system's internal state based solely on its external outputs. Derived from control theory in engineering, observability in software systems measures how well internal conditions can be inferred from data that the system exposes. This concept has become essential in modern distributed architectures where traditional monitoring approaches fall short due to the complexity, scale, and dynamic nature of cloud-native applications. Observability enables teams to ask arbitrary questions about their system's behavior without having to predict what might go wrong in advance.

In the DevOps context, observability bridges the gap between development and operations by providing a shared understanding of system behavior across the entire software lifecycle. It supports key DevOps principles including fast feedback loops, continuous improvement, and shared responsibility. Unlike traditional monitoring, which focuses on predefined metrics and known failure modes, observability provides the context and data needed to investigate unpredicted issues in complex systems. This capability is particularly valuable in environments practicing continuous delivery, where rapid release cycles and dynamic infrastructure create constantly evolving systems. By enabling teams to quickly understand and resolve issues, observability helps maintain system reliability while supporting the velocity that DevOps practices aim to achieve.

> "Observability is a measure of how well internal states of a system can be inferred from knowledge of its external outputs." - [Wikipedia](https://en.wikipedia.org/wiki/Observability)

## The Three Pillars of Observability

Observability in software systems is typically implemented through three complementary data types, often referred to as the "three pillars of observability." **Logs** are timestamped records of discrete events that occur within a system. They provide detailed context about specific occurrences, including errors, warnings, and informational events. Structured logging, which organizes log data into consistent, parseable formats, enhances the usefulness of logs by making them more searchable and analyzable. While logs offer rich contextual information, they can become overwhelming in high-volume systems and may consume significant storage resources if not properly managed.

**Metrics** are numerical measurements collected over time that represent system behavior and performance. They include counters (values that increase over time, such as request counts), gauges (values that can increase or decrease, such as memory usage), and histograms (distributions of values, such as response time percentiles). Metrics are typically lightweight, enabling high-frequency collection with minimal performance impact. They excel at providing a broad overview of system health and performance trends, making them ideal for dashboards and alerting. However, metrics alone lack the detailed context needed to understand the root cause of complex issues.

**Traces** track the progression of requests as they flow through distributed systems, connecting the dots between different services and components. Each trace consists of spans that represent operations within individual services, creating a hierarchical view of the request's journey. Distributed tracing is particularly valuable in microservices architectures, where a single user request might traverse dozens of services. Traces provide crucial timing information about each component's performance and reveal dependencies between services. By connecting traces with logs and metrics, teams can gain a comprehensive understanding of system behavior, moving from symptom detection (through metrics) to contextual understanding (through logs) to causal analysis (through traces).

## Good Practices

Implement structured logging with consistent formats and appropriate detail levels, including correlation IDs that allow events to be associated with specific requests across distributed systems.

Design metrics collection with both technical and business perspectives in mind, focusing on the golden signals (latency, traffic, errors, and saturation) while also tracking metrics that directly relate to user experience and business outcomes.

Adopt a service-level objective (SLO) approach that defines clear, measurable reliability targets based on user experience, using observability data to track performance against these objectives.

Implement automated anomaly detection that leverages machine learning to identify unusual patterns in observability data, reducing alert fatigue and helping teams focus on genuine issues.

Build observability into applications from the beginning rather than adding it as an afterthought, treating instrumentation as a first-class concern during development and making observability a shared responsibility across development and operations teams.

## Further Reading

* [Distributed Systems Observability](https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/) by Cindy Sridharan - Comprehensive guide to the principles and practices of observability in modern distributed systems

* [Observability Engineering](https://www.oreilly.com/library/view/observability-engineering/9781492076438/) by Charity Majors, Liz Fong-Jones, and George Miranda - Practical approaches to achieving production excellence through observability

* [Cloud Native Observability with OpenTelemetry](https://www.packtpub.com/product/cloud-native-observability-with-opentelemetry/9781801077705) by Alex Boten - Guide to implementing standardized observability using the OpenTelemetry framework

* [SRE Workbook](https://sre.google/workbook/table-of-contents/) by Google - Practical guidance on implementing observability as part of site reliability engineering practices

* [Practical Monitoring](https://www.oreilly.com/library/view/practical-monitoring/9781491957349/) by Mike Julian - Strategies for building effective monitoring and observability systems with real-world examples
